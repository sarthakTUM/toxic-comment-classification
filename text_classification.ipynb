{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.utils import any2unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump(data, filename):\n",
    "    with open(filename + \".pickle\", \"wb\") as output_file:\n",
    "        cPickle.dump(data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    with open(filename + \".pickle\", \"rb\") as input_file:\n",
    "        e = cPickle.load(input_file)\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6              COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK\n",
       "42        You are gay or antisemmitian? \\n\\nArchangel WH...\n",
       "43                 FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!\n",
       "55        Stupid peace of shit stop deleting my stuff as...\n",
       "56        =Tony Sidaway is obviously a fistfuckee. He lo...\n",
       "65        All of my edits are good.  Cunts like you who ...\n",
       "86        Would you both shut up, you don't run wikipedi...\n",
       "105              A pair of jew-hating weiner nazi schmucks.\n",
       "168       You should be fired, you're a moronic wimp who...\n",
       "176       I think that your a Fagget get a oife and burn...\n",
       "179       REPLY ABOVE:\\nThat was me, loser. The UN defin...\n",
       "181       you are a stupid fuck \\n\\nand your mother's cu...\n",
       "211                   Fuck you, block me, you faggot pussy!\n",
       "218       Kill all niggers. \\n\\nI have hard, that others...\n",
       "231       Burn Deck \\n\\nIf that'd guy's burn deck is lik...\n",
       "238       u r a tw@ fuck off u gay boy.U r smelly.Fuck u...\n",
       "295       this user is such a worthless goddamn faggot f...\n",
       "312       \"\\n\\n Fuck you \\n\\n  Fuck you award go fuck yo...\n",
       "318       Keep your eye on it, no one cares, OK? People ...\n",
       "344       Fuck you \\n\\n-User:The Bohemian Shinobis yeah,...\n",
       "392       Dear Mokele,\\nYou have no right to tell people...\n",
       "415       Thank you for your RACIST experimenting with t...\n",
       "429       Gay \\n\\nThe existence of CDVF is further proof...\n",
       "437       I NEVER FUCKING MADE THIS MOTHER FUCKING ARTIC...\n",
       "442            Hi \\n\\nIm a fucking bitch.\\n\\n50.180.208.181\n",
       "476       F**K ALL RIDES AT WORLDS OF FUN  \\n\\nI hate al...\n",
       "497       \"\"\"Nazi filth\"\" is impolite  04:27, 20 Jan 200...\n",
       "521       Dictionaries\\n\\nHow dare you call my contribut...\n",
       "579       What a motherfucking piece of crap those fuckh...\n",
       "582                 You sir are an imbecile, and a pervert.\n",
       "                                ...                        \n",
       "158933    Administratore: I need your help to stop DIREK...\n",
       "158940                      kiss my ass you limp dick loser\n",
       "158951    FUCK YOU \\n\\nBARACK OBAMA IS A MUSLIM, HE SUCK...\n",
       "158984    hi,idiot, why are you delate my talking, just ...\n",
       "159033         Shut up you faggot.  4 July 2005 01:23 (UTC)\n",
       "159041    \"\\nARE YOU FUCKING RETARDED?\\n\\nPlease do not ...\n",
       "159045                                 Shut up, you button!\n",
       "159069                    You suck bitch \\n\\nO'DOYLE RULES!\n",
       "159096    You filthy, stinking crow! \\n\\nI am back! Dirt...\n",
       "159099    now\\n\\nyou fucking pathetic moron. \\n\\ncc:  Ji...\n",
       "159163    Shit \\n\\nkeep your sorry ass shit post off my ...\n",
       "159207    Don't allow yourself to be intimidated by Alex...\n",
       "159268    Hi Bading \\nPutang ina mong bakla ka. Fuck you...\n",
       "159281    LoL!! \\n\\nyou're GAY!! you will never know how...\n",
       "159290    Hey alabamoy boy why dont you stick your head ...\n",
       "159298             , are you dumber than you look? asshole.\n",
       "159312    Walter Mercado \\n\\nAntonio, quite frankly, you...\n",
       "159336    Oh, fuck off. The pansy Jew would just whine a...\n",
       "159342    Fuck off turd. Don't ever ban me again you cun...\n",
       "159368    \"\\n\\n Palin/Satan 2012 \\n\\nWow, what a surpris...\n",
       "159378    GO AHEAD AND FUCKING BAN ME ~ LIKE THAT WILL H...\n",
       "159382                        shut up you goddamn assclown.\n",
       "159400    Shalom \\n\\nSemite, get the fuck out of here. I...\n",
       "159411    Fat piece of shit \\n\\nyou obese piece of shit....\n",
       "159493                           FUCKING FAGGOT \\n\\nLOLWAT.\n",
       "159494    \"\\n\\n our previous conversation \\n\\nyou fuckin...\n",
       "159514                    YOU ARE A MISCHIEVIOUS PUBIC HAIR\n",
       "159541    Your absurd edits \\n\\nYour absurd edits on gre...\n",
       "159546    \"\\n\\nHey listen don't you ever!!!! Delete my e...\n",
       "159554    and i'm going to keep posting the stuff u dele...\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.insult==1]['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for nulls\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              0\n",
       "comment_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic:  0    144277\n",
      "1     15294\n",
      "Name: toxic, dtype: int64\n",
      "severe_toxic:  0    157976\n",
      "1      1595\n",
      "Name: severe_toxic, dtype: int64\n",
      "obscene:  0    151122\n",
      "1      8449\n",
      "Name: obscene, dtype: int64\n",
      "threat:  0    159093\n",
      "1       478\n",
      "Name: threat, dtype: int64\n",
      "insult:  0    151694\n",
      "1      7877\n",
      "Name: insult, dtype: int64\n",
      "identity hate:  0    158166\n",
      "1      1405\n",
      "Name: identity_hate, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for imbalance classes\n",
    "print 'toxic: ', train.toxic.value_counts()\n",
    "print 'severe_toxic: ', train.severe_toxic.value_counts()\n",
    "print 'obscene: ', train.obscene.value_counts()\n",
    "print 'threat: ', train.threat.value_counts()\n",
    "print 'insult: ', train.insult.value_counts()\n",
    "print 'identity hate: ', train.identity_hate.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_vectors_ndarray = load('test_vectors_ndarray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vectors_ndarray = load('train_vectors_ndarray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = train[['id', 'comment_text']]\n",
    "y_train = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...\n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...\n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...\n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...\n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0      0             0        0       0       0              0\n",
       "1      0             0        0       0       0              0\n",
       "2      0             0        0       0       0              0\n",
       "3      0             0        0       0       0              0\n",
       "4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.as_matrix().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - 6L MLP with Adam: 0.9604 public LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = Sequential()\n",
    "mlp.add(Dense(400, input_shape=(300,), activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(200, activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(100, activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(50, activation='relu'))\n",
    "mlp.add(Dropout(0.5))\n",
    "mlp.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "mlp.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path=\"weights_base_6L_adam.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/100\n",
      "126464/127656 [============================>.] - ETA: 0s - loss: 0.1218 - acc: 0.9528Epoch 00000: val_loss improved from inf to 0.06408, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 3s - loss: 0.1213 - acc: 0.9530 - val_loss: 0.0641 - val_acc: 0.9769\n",
      "Epoch 2/100\n",
      "125440/127656 [============================>.] - ETA: 0s - loss: 0.0723 - acc: 0.9766Epoch 00001: val_loss improved from 0.06408 to 0.06245, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0722 - acc: 0.9766 - val_loss: 0.0625 - val_acc: 0.9786\n",
      "Epoch 3/100\n",
      "126464/127656 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9775Epoch 00002: val_loss improved from 0.06245 to 0.06202, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0686 - acc: 0.9775 - val_loss: 0.0620 - val_acc: 0.9784\n",
      "Epoch 4/100\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9779Epoch 00003: val_loss improved from 0.06202 to 0.06027, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0666 - acc: 0.9779 - val_loss: 0.0603 - val_acc: 0.9790\n",
      "Epoch 5/100\n",
      "125440/127656 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9782Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0656 - acc: 0.9782 - val_loss: 0.0626 - val_acc: 0.9784\n",
      "Epoch 6/100\n",
      "124928/127656 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9784Epoch 00005: val_loss improved from 0.06027 to 0.05890, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0645 - acc: 0.9784 - val_loss: 0.0589 - val_acc: 0.9796\n",
      "Epoch 7/100\n",
      "125696/127656 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9784Epoch 00006: val_loss improved from 0.05890 to 0.05861, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0641 - acc: 0.9784 - val_loss: 0.0586 - val_acc: 0.9797\n",
      "Epoch 8/100\n",
      "125184/127656 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9787Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0631 - acc: 0.9787 - val_loss: 0.0588 - val_acc: 0.9793\n",
      "Epoch 9/100\n",
      "125696/127656 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9787Epoch 00008: val_loss improved from 0.05861 to 0.05812, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0625 - acc: 0.9787 - val_loss: 0.0581 - val_acc: 0.9795\n",
      "Epoch 10/100\n",
      "124928/127656 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9790Epoch 00009: val_loss improved from 0.05812 to 0.05810, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0619 - acc: 0.9790 - val_loss: 0.0581 - val_acc: 0.9797\n",
      "Epoch 11/100\n",
      "125952/127656 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9790Epoch 00010: val_loss improved from 0.05810 to 0.05806, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0620 - acc: 0.9790 - val_loss: 0.0581 - val_acc: 0.9796\n",
      "Epoch 12/100\n",
      "126720/127656 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9792Epoch 00011: val_loss improved from 0.05806 to 0.05770, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0610 - acc: 0.9791 - val_loss: 0.0577 - val_acc: 0.9799\n",
      "Epoch 13/100\n",
      "126720/127656 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9793Epoch 00012: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0606 - acc: 0.9793 - val_loss: 0.0579 - val_acc: 0.9796\n",
      "Epoch 14/100\n",
      "125440/127656 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9792Epoch 00013: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0605 - acc: 0.9792 - val_loss: 0.0594 - val_acc: 0.9790\n",
      "Epoch 15/100\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9793Epoch 00014: val_loss improved from 0.05770 to 0.05709, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0599 - acc: 0.9793 - val_loss: 0.0571 - val_acc: 0.9798\n",
      "Epoch 16/100\n",
      "126976/127656 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9794Epoch 00015: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0596 - acc: 0.9794 - val_loss: 0.0583 - val_acc: 0.9795\n",
      "Epoch 17/100\n",
      "127232/127656 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9795Epoch 00016: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0594 - acc: 0.9795 - val_loss: 0.0572 - val_acc: 0.9797\n",
      "Epoch 18/100\n",
      "126464/127656 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9795Epoch 00017: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0591 - acc: 0.9795 - val_loss: 0.0573 - val_acc: 0.9800\n",
      "Epoch 19/100\n",
      "125696/127656 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9797Epoch 00018: val_loss improved from 0.05709 to 0.05641, saving model to weights_base_6L_adam.best.hdf5\n",
      "127656/127656 [==============================] - 2s - loss: 0.0587 - acc: 0.9797 - val_loss: 0.0564 - val_acc: 0.9802\n",
      "Epoch 20/100\n",
      "125184/127656 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9797Epoch 00019: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0586 - acc: 0.9797 - val_loss: 0.0572 - val_acc: 0.9799\n",
      "Epoch 21/100\n",
      "125952/127656 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9797Epoch 00020: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0584 - acc: 0.9797 - val_loss: 0.0570 - val_acc: 0.9801\n",
      "Epoch 22/100\n",
      "126720/127656 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9799Epoch 00021: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0581 - acc: 0.9799 - val_loss: 0.0573 - val_acc: 0.9799\n",
      "Epoch 23/100\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9797Epoch 00022: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0581 - acc: 0.9797 - val_loss: 0.0586 - val_acc: 0.9799\n",
      "Epoch 24/100\n",
      "125184/127656 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9798Epoch 00023: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0575 - acc: 0.9798 - val_loss: 0.0580 - val_acc: 0.9799\n",
      "Epoch 25/100\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9798Epoch 00024: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0576 - acc: 0.9798 - val_loss: 0.0572 - val_acc: 0.9801\n",
      "Epoch 26/100\n",
      "126464/127656 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9798Epoch 00025: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0578 - acc: 0.9798 - val_loss: 0.0583 - val_acc: 0.9799\n",
      "Epoch 27/100\n",
      "127232/127656 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9799Epoch 00026: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0573 - acc: 0.9799 - val_loss: 0.0568 - val_acc: 0.9802\n",
      "Epoch 28/100\n",
      "125184/127656 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9800Epoch 00027: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0571 - acc: 0.9800 - val_loss: 0.0567 - val_acc: 0.9803\n",
      "Epoch 29/100\n",
      "124672/127656 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9799Epoch 00028: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0572 - acc: 0.9799 - val_loss: 0.0571 - val_acc: 0.9799\n",
      "Epoch 30/100\n",
      "125440/127656 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9800Epoch 00029: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0568 - acc: 0.9801 - val_loss: 0.0569 - val_acc: 0.9800\n",
      "Epoch 31/100\n",
      "125184/127656 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9802Epoch 00030: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0565 - acc: 0.9801 - val_loss: 0.0571 - val_acc: 0.9801\n",
      "Epoch 32/100\n",
      "126464/127656 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9802Epoch 00031: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0566 - acc: 0.9802 - val_loss: 0.0583 - val_acc: 0.9798\n",
      "Epoch 33/100\n",
      "126208/127656 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9799Epoch 00032: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0568 - acc: 0.9799 - val_loss: 0.0570 - val_acc: 0.9801\n",
      "Epoch 34/100\n",
      "125696/127656 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9800Epoch 00033: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0563 - acc: 0.9801 - val_loss: 0.0571 - val_acc: 0.9800\n",
      "Epoch 35/100\n",
      "125696/127656 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9802Epoch 00034: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0561 - acc: 0.9802 - val_loss: 0.0567 - val_acc: 0.9799\n",
      "Epoch 36/100\n",
      "124672/127656 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9803Epoch 00035: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0557 - acc: 0.9803 - val_loss: 0.0581 - val_acc: 0.9800\n",
      "Epoch 37/100\n",
      "126720/127656 [============================>.] - ETA: 0s - loss: 0.0559 - acc: 0.9802Epoch 00036: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0558 - acc: 0.9802 - val_loss: 0.0581 - val_acc: 0.9800\n",
      "Epoch 38/100\n",
      "124928/127656 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9803Epoch 00037: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0559 - acc: 0.9803 - val_loss: 0.0580 - val_acc: 0.9793\n",
      "Epoch 39/100\n",
      "126720/127656 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9804Epoch 00038: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0553 - acc: 0.9804 - val_loss: 0.0576 - val_acc: 0.9802\n",
      "Epoch 40/100\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9803Epoch 00039: val_loss did not improve\n",
      "127656/127656 [==============================] - 2s - loss: 0.0553 - acc: 0.9804 - val_loss: 0.0568 - val_acc: 0.9802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35a25c0790>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(train_vectors_ndarray, y_train.as_matrix(), nb_epoch=100, batch_size=256, verbose=1, validation_split=0.2,\n",
    "       callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU: 0.9828 public LB, 622 Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best GRU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_7 (Embedding)          (None, 250, 200)      20000000    embedding_input_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional)  (None, 250, 256)      252672      embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "globalmaxpooling1d_1 (GlobalMaxP (None, 256)           0           bidirectional_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)             (None, 256)           0           globalmaxpooling1d_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_13 (Dense)                 (None, 128)           32896       dropout_18[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)             (None, 128)           0           dense_13[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_14 (Dense)                 (None, 6)             774         dropout_19[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 20,286,342\n",
      "Trainable params: 286,342\n",
      "Non-trainable params: 20,000,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:\n",
    "\n",
    "dropout_U = 0.1\n",
    "\n",
    "dropout_W = 0.1\n",
    "\n",
    "dropout_dense1 = 0.1\n",
    "\n",
    "max_features = 100000\n",
    "\n",
    "max_len = 250\n",
    "\n",
    "vector_len = 200\n",
    "\n",
    "batch size= 512\n",
    "\n",
    "validation: 0.1\n",
    "\n",
    "Epoch 1/100\n",
    "143360/143613 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9863Epoch 00000: val_loss improved from 0.04145 to 0.04144, saving model to weights_gru_128BiGru_dropouts0pt1_dense128_BS512_adam.best.hdf5\n",
    "\n",
    " ROC-AUC - epoch: 1 - score: 0.988243 \n",
    "\n",
    "143613/143613 [==============================] - 261s - loss: 0.0345 - acc: 0.9863 - val_loss: 0.0414 - val_acc: 0.9844"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXPERIMENTS:\n",
    "\n",
    "1. Bi GRU with 256 Dense and 64 GRU with 0.1 dropout and 32 BS yielded 0.9814 accuracy, max_len=250\n",
    "\n",
    "2. Bi GRU with 128 Dense and 64 GRU with 0.2 dropout and 256 BS yielded 0.9815 accuracy, max_len=250\n",
    "\n",
    "3. Bi GRU with 128 Dense and 64 GRU with 0.1 dropout and 512 BS yielded 0.9828 accuracy, max_len=250. Increasing  batch size decreases training time, does not reduce score. 0.1 dropout is good, as well as 128 dense. Higher values for both reduced score. \n",
    "\n",
    "4. Batch Normalization after Dropout in Dense did not help, reduced score\n",
    "\n",
    "5. increasing dropouts to 0.5 was not at all good\n",
    "\n",
    "6. increasing dropout to 0.2 with BS 512 and CNN+GRU and LR exp decay produced local AUC of 0.9870 but 0.9810 on public LB. - keep it for final submission\n",
    "\n",
    "7. BS32 with CNN+GRU reduced the public LB to 0.9732 after 6 epochs\n",
    "\n",
    "8. CNN after GRU is not good. increasing GRU output decreased val_loss and lead to 0.9831 on public LB. The network was further trained using 0.0001 LR with exp decay. Maybe try reducing the LR further, or train complete model using LR with exp decay.\n",
    "\n",
    "9. Bi-GRU 128 with dense 128 trained with exp_decay achieved good CV ROC_AUC, but scored .9831 on public LB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE='/media/sarthak/HDD/data_science/Kaggle/jigsaw_toxic/glove.twitter.27B.200d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.75499982e-02,  -7.29900002e-02,   5.42400002e-01,\n",
       "         7.70530030e-02,   1.68520004e-01,   4.20570001e-02,\n",
       "        -6.92030013e-01,  -5.26000023e-01,  -3.75719994e-01,\n",
       "        -7.64179975e-02,   2.78530002e-01,   5.62749982e-01,\n",
       "        -1.43410003e+00,   4.01129991e-01,  -3.78769994e-01,\n",
       "        -2.71349996e-01,   2.49200001e-01,  -5.66839993e-01,\n",
       "        -3.79669994e-01,  -3.82070005e-01,  -1.13830000e-01,\n",
       "        -2.23000005e-01,   4.32830006e-02,   2.27819994e-01,\n",
       "         3.83630008e-01,   9.02469993e-01,   1.81920007e-01,\n",
       "        -7.07949977e-03,  -2.51349986e-01,  -4.29110005e-02,\n",
       "        -5.85730016e-01,  -8.31459984e-02,   2.34549999e-01,\n",
       "        -3.40930015e-01,   4.00620013e-01,  -1.95230007e-01,\n",
       "         1.27010003e-01,   3.14830005e-01,   2.87770003e-01,\n",
       "         2.64070004e-01,  -3.61030012e-01,  -5.60890019e-01,\n",
       "        -2.80950010e-01,   5.09440005e-01,   2.81580001e-01,\n",
       "        -6.95409998e-02,   2.37440005e-01,  -6.76280037e-02,\n",
       "        -2.67230004e-01,  -2.26860002e-01,  -2.81989992e-01,\n",
       "        -5.57489991e-01,   1.69210002e-01,  -2.28259996e-01,\n",
       "        -1.07179999e+00,  -3.55540007e-01,  -1.60459995e-01,\n",
       "        -2.03700006e-01,  -6.87139988e-01,   2.46419996e-01,\n",
       "        -4.89030004e-01,   1.87590003e-01,  -2.77819991e-01,\n",
       "        -6.27349973e-01,   5.28410017e-01,   2.86020011e-01,\n",
       "         5.70749998e-01,   3.64650011e-01,  -1.88309997e-01,\n",
       "        -2.78789997e-01,  -5.94969988e-01,   3.77420008e-01,\n",
       "         1.87230006e-01,  -3.02089989e-01,   2.89860010e-01,\n",
       "         6.16090000e-01,  -6.04089975e-01,  -4.38959986e-01,\n",
       "        -5.51429996e-03,   1.91019999e-03,   7.14190006e-01,\n",
       "         1.67789996e-01,   2.88869999e-02,  -9.80570018e-02,\n",
       "         1.85499996e-01,   1.32159993e-01,  -3.67810018e-02,\n",
       "        -4.21770006e-01,   7.73029998e-02,  -7.03869984e-02,\n",
       "         3.25619996e-01,  -1.63780004e-01,   4.73569989e-01,\n",
       "        -4.10670012e-01,  -1.18629999e-01,  -2.28249997e-01,\n",
       "         4.20060009e-01,  -8.38479996e-02,   4.19710010e-01,\n",
       "        -7.57889971e-02,   2.19060004e-01,  -1.65089995e-01,\n",
       "         4.45129991e-01,  -2.31800005e-01,   2.58969992e-01,\n",
       "        -2.53969997e-01,   3.48030001e-01,  -5.68469986e-02,\n",
       "        -8.14429998e-01,  -5.37989974e-01,   5.95730007e-01,\n",
       "         2.02639997e-01,   5.91909997e-02,  -2.00450003e-01,\n",
       "         2.12640002e-01,  -1.88069999e-01,  -2.14900002e-01,\n",
       "        -3.44909996e-01,  -2.65309989e-01,  -2.72439986e-01,\n",
       "        -3.20630014e-01,  -3.31070006e-01,  -3.75440001e-01,\n",
       "        -1.77929997e-01,  -7.88889974e-02,   3.74480009e-01,\n",
       "        -4.03459996e-01,   1.28539994e-01,   3.65579993e-01,\n",
       "         2.92470008e-01,   1.13739997e-01,  -6.63480014e-02,\n",
       "        -5.12369983e-02,   4.82230008e-01,   3.08670014e-01,\n",
       "        -2.15529993e-01,  -6.94819987e-02,  -9.94599983e-02,\n",
       "        -1.72339994e-02,   7.95210004e-02,   6.23329997e-01,\n",
       "        -2.06750005e-01,  -6.28180027e-01,   6.61870003e-01,\n",
       "        -5.03950000e-01,   5.22469997e-01,   4.43859994e-01,\n",
       "         5.95870018e-01,  -1.31559996e-02,   3.15739989e-01,\n",
       "         3.36860001e-01,   3.15019995e-01,  -6.34450006e+00,\n",
       "        -1.19329996e-01,   9.74720001e-01,  -3.46460015e-01,\n",
       "         1.78020000e-01,  -3.09830010e-01,  -4.32579994e-01,\n",
       "         5.23609996e-01,   4.10609990e-01,   5.04170001e-01,\n",
       "        -1.36160001e-01,   7.53220022e-02,   7.32270002e-01,\n",
       "        -7.35689998e-02,  -3.10860008e-01,  -1.13990001e-01,\n",
       "         4.44260001e-01,  -3.12539995e-01,  -1.63700003e-02,\n",
       "         1.77529994e-02,   6.88139975e-01,   2.87389994e-01,\n",
       "         1.32420003e-01,   1.76459998e-01,   6.05840027e-01,\n",
       "         5.34979999e-01,  -2.26669997e-01,  -7.38979995e-01,\n",
       "        -8.31239969e-02,   2.80719995e-01,   4.89129990e-01,\n",
       "        -1.45539999e-01,  -5.09320021e-01,   6.33509994e-01,\n",
       "         2.74450004e-01,  -4.75120008e-01,  -1.59060001e-01,\n",
       "        -4.32080001e-01,   4.43990007e-02,  -5.38299978e-01,\n",
       "        -2.99849987e-01,   1.02210000e-01,  -2.75310010e-01,\n",
       "         1.99039996e-01,  -4.65720007e-03,   3.13859992e-02,\n",
       "         3.55309993e-01,  -4.48469996e-01], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['in'] # random word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train.comment_text\n",
    "list_sentences_test = test.comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_unique_words = 100000\n",
    "max_len = 250\n",
    "tokenizer = Tokenizer(nb_words=max_unique_words)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"dool's\": 44257,\n",
       " '\\xca\\x8a': 96087,\n",
       " 'sowell': 65677,\n",
       " 'tsukino': 96089,\n",
       " '\\xca\\x84': 96090,\n",
       " \"agencies'\": 202376,\n",
       " 'woods': 8638,\n",
       " 'spiders': 27398,\n",
       " 'gavan': 65678,\n",
       " 'dekolb': 47304,\n",
       " 'ultimatley': 96092,\n",
       " 'woody': 15188,\n",
       " 'trawling': 22036,\n",
       " 'comically': 44258,\n",
       " '027597675': 96093,\n",
       " '\\xe2\\x80\\x9emerkspruch\\xe2\\x80\\x9d': 159400,\n",
       " 'regularize': 96094,\n",
       " 'alwiqi': 96095,\n",
       " 'eastasia': 65679,\n",
       " 'sprague': 65680,\n",
       " 'caney': 65688,\n",
       " 'jairam': 96096,\n",
       " 'acurately': 96097,\n",
       " 'supasoldier': 96098,\n",
       " 'falseinformation': 172137,\n",
       " '\\xe2\\x80\\x98west\\xe2\\x80\\x99': 96099,\n",
       " 'rickman': 96100,\n",
       " 'foundation\\xe2\\x80\\x99s': 96101,\n",
       " 'riconferma': 52273,\n",
       " 'dra\\xc5\\xbea': 96102,\n",
       " 'riconferme': 65698,\n",
       " '\\xd0\\xa2\\xd0\\xb5\\xd0\\xbe\\xd0\\xb4\\xd0\\xbe\\xd1\\x80': 175594,\n",
       " 'naturopathic': 20228,\n",
       " \"wood'\": 96104,\n",
       " 'sidestrand': 44259,\n",
       " 'pigment': 38869,\n",
       " 'occops': 96105,\n",
       " '41f1972e': 204714,\n",
       " 'tijfo098': 96106,\n",
       " 'bringing': 2801,\n",
       " 'raviah': 96107,\n",
       " 'jrpg': 52274,\n",
       " 'tcby': 96108,\n",
       " 'wooded': 52275,\n",
       " 'distributerd': 75657,\n",
       " 'kangema': 96109,\n",
       " 'wooden': 14508,\n",
       " 'kacian': 65681,\n",
       " 'wednesday': 9016,\n",
       " 'virtuosos': 96111,\n",
       " 'chameleons': 52276,\n",
       " 'elgar': 65682,\n",
       " 'amplifications': 96112,\n",
       " 'immunities': 96113,\n",
       " 'tetrahedron': 70634,\n",
       " 'sekgathe': 96115,\n",
       " 'thrace': 14143,\n",
       " 'thecyclothymiacollective': 96116,\n",
       " 'jmorrison230582': 96117,\n",
       " '\\xca\\xb0': 114704,\n",
       " 'estonisans': 96119,\n",
       " 'thraco': 34840,\n",
       " '270': 18128,\n",
       " '271': 29429,\n",
       " '272': 34841,\n",
       " '273': 29430,\n",
       " '274': 29431,\n",
       " '275': 22037,\n",
       " '276': 31897,\n",
       " '277': 18781,\n",
       " '278': 34842,\n",
       " '279': 44260,\n",
       " '16701': 96120,\n",
       " 'tumen': 65683,\n",
       " 'bronte': 42449,\n",
       " 'dialogs': 96121,\n",
       " 'jetstreamer': 38870,\n",
       " \"screamin'\": 96122,\n",
       " 'gayyyyyyyyyyy': 96123,\n",
       " 'opportunists': 65684,\n",
       " '\\xd0\\x97\\xd0\\xb0\\xd0\\xb1\\xd1\\x83\\xd0\\xb4\\xd1\\x8c': 96124,\n",
       " 'warmongering': 38871,\n",
       " 'usenet': 8911,\n",
       " 'jadams7831': 96125,\n",
       " \"kid'\": 65685,\n",
       " 'dialogo': 96126,\n",
       " 'numeral': 10960,\n",
       " \"'pleasure\": 96127,\n",
       " 'beejaypii': 96128,\n",
       " '27d': 96129,\n",
       " 'lench': 96130,\n",
       " '\\xe2\\x80\\x94anonymous': 96131,\n",
       " 'mailings': 29432,\n",
       " 'dayumm': 96132,\n",
       " 'bhubaneshwar': 96133,\n",
       " 'conrtibs': 153270,\n",
       " 'affiliates': 13502,\n",
       " 'quagmires': 52278,\n",
       " '27s': 11839,\n",
       " '27t': 34843,\n",
       " 'affiliated': 4377,\n",
       " 'ymself': 96134,\n",
       " '\\xd8\\xb3\\xd8\\xa7\\xd8\\xae\\xd8\\xaa\\xd9\\x85': 96135,\n",
       " 'vivo': 80482,\n",
       " 'kshetra': 190609,\n",
       " 'kids': 2354,\n",
       " 'uplifting': 34844,\n",
       " '\\xe2\\x80\\x9cgreat': 96136,\n",
       " 'kidz': 65687,\n",
       " 'deferring': 38872,\n",
       " 'controversy': 1255,\n",
       " 'kida': 96137,\n",
       " 'kidd': 27399,\n",
       " 'nomader': 108037,\n",
       " 'neurologist': 29433,\n",
       " 'bulltwang': 96138,\n",
       " 'ircname': 96139,\n",
       " 'kido': 96140,\n",
       " 'topography': 25747,\n",
       " \"ocr'd\": 65689,\n",
       " 'beans\\xc2\\xa0': 44262,\n",
       " 'unsinkable': 65691,\n",
       " 'tamperings': 96142,\n",
       " 'sneha': 96144,\n",
       " 'dnd': 96145,\n",
       " 'rebeb': 96146,\n",
       " 'dna': 4058,\n",
       " 'dnc': 31895,\n",
       " 'dnb': 15597,\n",
       " 'dnl': 52279,\n",
       " 'dni': 52280,\n",
       " 'dnh': 96147,\n",
       " 'sidebars': 52281,\n",
       " 'dnt': 22039,\n",
       " 'refunding': 153276,\n",
       " 'dns': 18782,\n",
       " 'sidebark': 96148,\n",
       " 'dny': 96149,\n",
       " 'my2c': 96150,\n",
       " 'statsmicro': 96151,\n",
       " 'hariakhan': 31896,\n",
       " \"johtex's\": 96152,\n",
       " '\\xe2\\x80\\xab\\xc2\\xb7\\xe2\\x80\\x8f\\xd7\\x9c\\xd7\\xa2\\xd7\\xa8\\xd7\\x99': 96153,\n",
       " 'populations': 5168,\n",
       " \"mihaiam's\": 96154,\n",
       " \"'criticism\": 65692,\n",
       " 'yahoo': 3462,\n",
       " 'benedikz': 96155,\n",
       " 'meteorologist': 65693,\n",
       " 'expeditionary': 21114,\n",
       " 'pol\\xc3\\xadcia': 85225,\n",
       " 'kosovothanksyou': 96157,\n",
       " 'whoisnegiafk': 96158,\n",
       " 'voicework': 65694,\n",
       " 'gija': 34845,\n",
       " 'agelastes': 65695,\n",
       " 'archieves': 17536,\n",
       " \"pilgrims'\": 96159,\n",
       " \"'saturn'\": 96160,\n",
       " 'sylviawatsoncampaign': 65696,\n",
       " 'dusanr': 96161,\n",
       " \"'kate\": 96162,\n",
       " 'orrelon': 52282,\n",
       " 'steinweg': 96163,\n",
       " 'gershom': 52283,\n",
       " 'writingsreposted': 96164,\n",
       " 'archieved': 96165,\n",
       " 'sectarianism': 21115,\n",
       " \"post's\": 44264,\n",
       " 'subgenus': 52284,\n",
       " 'hyatt': 65697,\n",
       " 'bhakkar': 49964,\n",
       " 'embarressing': 96166,\n",
       " 'titanium': 19464,\n",
       " \"propeller's\": 96103,\n",
       " 'pinto': 19465,\n",
       " 'reshifting': 96167,\n",
       " 'ferranti': 206272,\n",
       " 'grafkm': 96168,\n",
       " 'duniya': 96169,\n",
       " 'tchaikovsky': 44265,\n",
       " 'beyer': 52286,\n",
       " 'memuriyetine': 96171,\n",
       " \"blocker's\": 172153,\n",
       " 'krait': 96172,\n",
       " 'exelent': 96173,\n",
       " 'materialscientist': 14510,\n",
       " 'itnot': 194707,\n",
       " \"chapters'\": 44266,\n",
       " 'distintion': 65700,\n",
       " 'wheatear': 96174,\n",
       " 'wei\\xc3\\x9fe': 52342,\n",
       " 'shaitan': 52287,\n",
       " 'adelaidean': 96175,\n",
       " 'assimilated': 14511,\n",
       " 'dinosaurs': 13823,\n",
       " 'wrong': 240,\n",
       " 'stadtsfries': 96179,\n",
       " 'sentencing': 19466,\n",
       " \"'dodo'\": 38873,\n",
       " 'assimilates': 96180,\n",
       " '\\xd8\\xa7\\xd9\\x84\\xd8\\xaa\\xd8\\xb9\\xd9\\x88\\xd9\\x8a\\xd8\\xb6\\xd8\\xa7\\xd8\\xaa': 96181,\n",
       " 'violentus': 96183,\n",
       " 'shreya': 96184,\n",
       " 'a3ie': 96185,\n",
       " '\\xc3\\x96mer': 172158,\n",
       " 'somayachs': 96186,\n",
       " 'pa23': 96187,\n",
       " '18p': 200654,\n",
       " 'disengaging': 65702,\n",
       " \"citations'\": 65868,\n",
       " 'snugly': 96189,\n",
       " 'goldfeld': 52288,\n",
       " 'arvanitet': 65704,\n",
       " 'welcomed': 6206,\n",
       " 'mudslime': 96190,\n",
       " 'fremdk': 96191,\n",
       " 'textypu': 96192,\n",
       " 'stoicism': 65872,\n",
       " \"arafat's\": 44267,\n",
       " 'tillamook': 65705,\n",
       " 'editprotected': 96194,\n",
       " 'ekbatana': 96195,\n",
       " 'concurrency': 34846,\n",
       " 'linuxfs': 65706,\n",
       " 'boltons': 96196,\n",
       " 'duece': 96197,\n",
       " 'mudslims': 65707,\n",
       " 'welcomes': 13503,\n",
       " 'fir': 18129,\n",
       " 'fis': 65708,\n",
       " 'fip': 65709,\n",
       " 'unjuestified': 96198,\n",
       " 'fit': 1413,\n",
       " 'fiu': 96199,\n",
       " 'fiz': 65889,\n",
       " 'screaming': 7850,\n",
       " 'fix': 890,\n",
       " \"'et\": 191192,\n",
       " '1659\\xe2\\x80\\x931729': 65710,\n",
       " 'folate': 96202,\n",
       " 'fib': 65711,\n",
       " 'fic': 52290,\n",
       " 'fia': 44268,\n",
       " 'fif': 96203,\n",
       " 'fig': 16996,\n",
       " 'fid': 65712,\n",
       " 'loadpackage': 96204,\n",
       " 'fik': 96205,\n",
       " 'shinawatra': 34847,\n",
       " 'n\\xcc\\xaa': 96206,\n",
       " 'fin': 18130,\n",
       " 'fil': 34848,\n",
       " 'shively': 65713,\n",
       " 'weell': 96207,\n",
       " 'fir0002': 172162,\n",
       " \"d'ethnographie\": 96208,\n",
       " 'marrigies': 96209,\n",
       " 'vouchers': 96210,\n",
       " 'genelogical': 96211,\n",
       " 'shivam': 96212,\n",
       " 'maupiti': 96213,\n",
       " 'sove': 52291,\n",
       " 'orangedumbshit': 96214,\n",
       " 'scaglione': 96215,\n",
       " 'circulaion': 96216,\n",
       " 'castigating': 65716,\n",
       " 'troublous': 65717,\n",
       " 'bhashaindia': 96217,\n",
       " 'barton': 24383,\n",
       " 'nixie': 96218,\n",
       " 'folha': 96219,\n",
       " 'bartov': 96220,\n",
       " 'neonazis': 65718,\n",
       " 'ossewa': 194034,\n",
       " 'zhuravlyov': 96221,\n",
       " 'zuid': 96222,\n",
       " 'apoc2400': 191195,\n",
       " \"'will'\": 65719,\n",
       " 'afadsbad': 114727,\n",
       " 'parasites': 19467,\n",
       " 'childen': 96225,\n",
       " 'nattif': 65720,\n",
       " 'ferraton': 96226,\n",
       " '2day': 96227,\n",
       " 'pisot': 65721,\n",
       " 'hollymarie13': 96228,\n",
       " 'kown': 52292,\n",
       " \"custerwest's\": 96229,\n",
       " \"msg''\": 96230,\n",
       " 'golem': 96231,\n",
       " '90c0': 96232,\n",
       " 'mia\\xc5\\xad': 96234,\n",
       " 'mannerheim': 96235,\n",
       " 'golek': 65722,\n",
       " '\\xe2\\x80\\x9csigns': 63462,\n",
       " 'kmh03': 65723,\n",
       " 'humber': 27400,\n",
       " 'offendors': 191197,\n",
       " 'combinatorial': 96238,\n",
       " 'hatake': 65724,\n",
       " 'golez': 65725,\n",
       " 'guitarists': 14144,\n",
       " \"'chronicle'\": 96239,\n",
       " 'cementery': 96240,\n",
       " 'amac\\xc4\\xb1yla': 96241,\n",
       " 'xeus': 96242,\n",
       " 'abbott': 17537,\n",
       " 'rauni': 96243,\n",
       " 'billkitty': 191198,\n",
       " 'corelated': 96244,\n",
       " 'marioman': 44269,\n",
       " 'pumpkins': 27401,\n",
       " 'berwick': 44270,\n",
       " 'chickamauga': 172169,\n",
       " 'sys': 38874,\n",
       " 'diamonitirion': 186578,\n",
       " 'arri435x': 65727,\n",
       " 'sickeningly': 52293,\n",
       " 'envall': 44271,\n",
       " \"'chronicles\": 96246,\n",
       " '\\xd8\\xa7\\xd9\\x84\\xd9\\x82\\xd8\\xa7\\xd8\\xa8\\xd8\\xb6\\xd8\\xa9': 197228,\n",
       " 'mindbogglingly': 207515,\n",
       " 'periodization': 44272,\n",
       " 'farward': 65728,\n",
       " 'farware': 65729,\n",
       " 'momment': 65730,\n",
       " 'portmanteau': 20229,\n",
       " 'mendacity': 52294,\n",
       " 'whatsmore': 52295,\n",
       " 'jomegat': 65731,\n",
       " 'purpose': 957,\n",
       " 'loook': 65732,\n",
       " \"fatherland'\": 96247,\n",
       " '\\xe2\\x80\\x9ccornish\\xe2\\x80\\x9d': 70644,\n",
       " 'loool': 65733,\n",
       " 'stoneham': 65734,\n",
       " 'skanderbeg': 20230,\n",
       " 'callous': 18783,\n",
       " 'broiught': 96249,\n",
       " '0059': 153290,\n",
       " \"tafi's\": 65736,\n",
       " 'inrehabilitation': 78041,\n",
       " 'stylistics': 96250,\n",
       " 'megabytes': 52296,\n",
       " 'breathalyzer': 114735,\n",
       " 'olds': 10212,\n",
       " 'renovated': 96251,\n",
       " 'oldy': 96252,\n",
       " 'distastful': 96253,\n",
       " \"birthers'\": 96255,\n",
       " \"chuq's\": 52297,\n",
       " 'needed': 663,\n",
       " 'master': 2600,\n",
       " 'oldj': 96257,\n",
       " 'fuzziness': 96258,\n",
       " 'genesis': 5232,\n",
       " 'macmap': 96259,\n",
       " 'eastvanhalen': 96260,\n",
       " 'tiffanymichelle': 96261,\n",
       " 'templarion': 153296,\n",
       " 'onlythetruth': 96263,\n",
       " 'motionless': 39759,\n",
       " 'summercamp': 96264,\n",
       " \"fischer's\": 52298,\n",
       " 'positively': 6726,\n",
       " 'condesending': 96265,\n",
       " 'ahmed': 9317,\n",
       " 'old2': 96266,\n",
       " 'themesotheliomasociety': 34849,\n",
       " 'codsall': 96267,\n",
       " 'duckworth': 13824,\n",
       " 'noah30': 52299,\n",
       " 'aaronshavit': 191104,\n",
       " 'radials': 74987,\n",
       " \"helm's\": 96268,\n",
       " 'ogame': 65739,\n",
       " 'ahmet': 44274,\n",
       " 'exclaimed': 52301,\n",
       " 'biolgy': 65740,\n",
       " \"old'\": 96269,\n",
       " 'silky': 75322,\n",
       " 'visces': 96270,\n",
       " 'roguephantom': 96271,\n",
       " 'pagurus': 96272,\n",
       " 'sh1t': 52277,\n",
       " 'miscellanea': 44340,\n",
       " \"thoth's\": 96273,\n",
       " 'andyvphil': 96274,\n",
       " 'hetrosexual': 65742,\n",
       " 'consenting': 52303,\n",
       " 'bogan': 96275,\n",
       " 'mishra': 34850,\n",
       " 'themepark': 96276,\n",
       " 'christianianity': 96277,\n",
       " '\\xcb\\x88\\xc9\\xb9o\\xca\\x8az\\xc9\\x99v\\xc9\\x9blt': 96278,\n",
       " 'credentialed': 52304,\n",
       " 'sitio': 96279,\n",
       " 'partridge': 34851,\n",
       " 'm\\xc4\\x81kun': 96280,\n",
       " 'microprinting': 96281,\n",
       " 'deepthroating': 96282,\n",
       " \"norwood's\": 96283,\n",
       " 'kremlin': 16480,\n",
       " 'rhcp': 52305,\n",
       " 'shipments': 25749,\n",
       " '45th': 34852,\n",
       " 'unknow': 96284,\n",
       " 'andromedae': 96285,\n",
       " 'represending': 96286,\n",
       " 'banditdos': 179022,\n",
       " 'arizonausa': 96287,\n",
       " \"ppe's\": 96288,\n",
       " 'reverence': 23154,\n",
       " 'functionary': 33869,\n",
       " '\\xd0\\x9a\\xd0\\xba\\xd1\\x80\\xd0\\xbe\\xd1\\x83\\xd0\\xbd\\xd0\\xbb': 96290,\n",
       " 'dailey': 65743,\n",
       " 'precisly': 65744,\n",
       " 'pseudonymously': 65745,\n",
       " 'paperland': 96291,\n",
       " 'majo': 44275,\n",
       " 'wessonjoe': 96292,\n",
       " 'jaranda': 25750,\n",
       " 'maja': 66054,\n",
       " 'ragnarok': 31899,\n",
       " 'tech': 4396,\n",
       " 'fugitives': 115821,\n",
       " 'intreast': 96294,\n",
       " 'maju': 96295,\n",
       " 'doesn\\xe2\\x80\\x99t': 6727,\n",
       " 'kuliglig': 96296,\n",
       " 'dickey': 96298,\n",
       " 'hounder': 96299,\n",
       " 'becus': 191209,\n",
       " 'tempter': 96300,\n",
       " 'vendalism': 52306,\n",
       " 'tempted': 7355,\n",
       " 'hounded': 24384,\n",
       " 'apace': 96301,\n",
       " 'darblick': 65746,\n",
       " 'bullshitted': 96302,\n",
       " 'naluboutes': 65747,\n",
       " 'mauriecoleman': 96303,\n",
       " \"'humor'\": 96304,\n",
       " '\\xe2\\x80\\x93\\xc2\\xa0\\xe2\\x80\\xa2': 96305,\n",
       " \"'supporting\": 65748,\n",
       " 'pawel': 65749,\n",
       " 'gurcu': 96306,\n",
       " 'kaiapoi': 96307,\n",
       " 'gurch': 18131,\n",
       " 'binkster': 96308,\n",
       " '3ltr': 65750,\n",
       " \"raul654's\": 65751,\n",
       " 'rickbartolucci': 65752,\n",
       " '0cdaq6aewaa': 96309,\n",
       " 'contentan': 210044,\n",
       " \"yopie's\": 65753,\n",
       " 'iata': 27402,\n",
       " 'nishkid': 22892,\n",
       " 'heptanoic': 96311,\n",
       " 'photoelectric': 44276,\n",
       " 'mandyh': 96313,\n",
       " 'karoch': 65754,\n",
       " 'attack\\xe2\\x80\\x99s': 96314,\n",
       " 'uncleanliness': 96315,\n",
       " 'jondel': 65755,\n",
       " 'dodbuzz': 96316,\n",
       " 'kcci': 96317,\n",
       " 'plushpuffin': 96319,\n",
       " 'wikihounding': 11650,\n",
       " 'yohimbe': 65756,\n",
       " 'patch': 11651,\n",
       " 'buttion': 96320,\n",
       " \"chocolat's\": 96321,\n",
       " 'keeeennnndddeyyyyy': 96322,\n",
       " 'conant': 46338,\n",
       " 'programmatic': 96323,\n",
       " 'uncomfirmed': 65757,\n",
       " 'agneskis': 96324,\n",
       " 'mansoorah': 96325,\n",
       " \"test's\": 52307,\n",
       " 'etic': 96326,\n",
       " 'zahler': 65758,\n",
       " 'aggrement': 96327,\n",
       " 'tyndale': 44277,\n",
       " 'tyndall': 96328,\n",
       " 'conani': 96329,\n",
       " 'etiq': 96330,\n",
       " \"o'mine\": 182425,\n",
       " 'necrid': 96331,\n",
       " 'gorillawarfare': 44278,\n",
       " 'advisedly': 65759,\n",
       " 'metasemiotics': 52308,\n",
       " 'epithelium': 96333,\n",
       " 'irs': 7010,\n",
       " 'irt': 65760,\n",
       " 'irv': 31900,\n",
       " 'wanyoike': 96335,\n",
       " 'iri': 52309,\n",
       " 'irk': 96336,\n",
       " 'irl': 13504,\n",
       " 'irm': 96337,\n",
       " 'irn': 96338,\n",
       " 'conductive': 38875,\n",
       " 'ira': 5189,\n",
       " 'irb': 29435,\n",
       " 'irc': 3212,\n",
       " 'ire': 23155,\n",
       " 'rschen7754': 65761,\n",
       " 'shaider': 96339,\n",
       " 'discipline': 5776,\n",
       " 'natura': 52310,\n",
       " \"'goodnight\": 176447,\n",
       " 'extend': 4823,\n",
       " 'nature': 1143,\n",
       " 'aftereffect': 202824,\n",
       " 'fruits': 10961,\n",
       " \"prdue's\": 96341,\n",
       " 'g\\xc3\\xb6k\\xc3\\xa7ek': 96342,\n",
       " '27n': 96343,\n",
       " 'extent': 2648,\n",
       " 'tendons': 96344,\n",
       " 'dalaina': 96345,\n",
       " \"seriously'\": 96346,\n",
       " 'klanken': 96347,\n",
       " 'b6rlitz': 96350,\n",
       " 'intercalate': 191220,\n",
       " 'artykuly': 96351,\n",
       " \"doubt'\": 65765,\n",
       " 'bzweebl': 96352,\n",
       " 'limitedly': 96353,\n",
       " 'seraphimblade': 29436,\n",
       " 'gopher': 31936,\n",
       " '\\xe2\\x82\\xacurotrash': 172189,\n",
       " 'likewsise': 96356,\n",
       " 'felicidad': 96357,\n",
       " 'knossos': 96358,\n",
       " 'garycompugeek': 96359,\n",
       " 'gambrel': 96360,\n",
       " 'humming': 44279,\n",
       " \"also's\": 96361,\n",
       " 'smackdown': 7927,\n",
       " 'triviality': 29437,\n",
       " 'finsbury': 96362,\n",
       " \"richie's\": 96363,\n",
       " 'finkel': 96364,\n",
       " 'choniates': 96365,\n",
       " 'obese': 15598,\n",
       " 'sensationalized': 52311,\n",
       " 'arkin': 96368,\n",
       " 'planecrashes': 96369,\n",
       " 'arkiv': 96370,\n",
       " 'administrativa': 114755,\n",
       " 'yugoslavians': 52312,\n",
       " 'budva': 36647,\n",
       " 'sensationalizes': 96372,\n",
       " 'doubts': 5233,\n",
       " 'poxleitner': 65768,\n",
       " 'heph': 65769,\n",
       " '1437244': 96373,\n",
       " 'barquq': 96374,\n",
       " 'propellants': 96375,\n",
       " 'fire\\xef\\xac\\x82y': 96376,\n",
       " \"admirers'\": 96378,\n",
       " \"oneguy's\": 38876,\n",
       " 'qbindex': 96379,\n",
       " 'baords': 65770,\n",
       " 'elv1s': 191226,\n",
       " 'professionally': 12245,\n",
       " 'pyrofork': 65771,\n",
       " 'misconstrued': 16481,\n",
       " '1672': 172196,\n",
       " 'ble2t': 96381,\n",
       " 'fundrasing': 96382,\n",
       " 'majid': 31901,\n",
       " 'faschism': 52313,\n",
       " 'unot': 96384,\n",
       " 'majic': 66191,\n",
       " 'canoeing': 65773,\n",
       " 'attributepov': 52314,\n",
       " 'majin': 29469,\n",
       " 'embargos': 96385,\n",
       " 'chang\\xe2\\x80\\x99s': 65775,\n",
       " 'meadlist': 96386,\n",
       " \"plants'\": 96387,\n",
       " 'unog': 96388,\n",
       " 'embargoe': 96389,\n",
       " 'gandhinagar': 96390,\n",
       " 'gunparade': 96391,\n",
       " 'pschemp': 29439,\n",
       " 'bricklayer': 96392,\n",
       " 'memorial': 5749,\n",
       " 'memoriam': 52315,\n",
       " 'grumpus': 96393,\n",
       " 'remand': 34855,\n",
       " 'memoriae': 29440,\n",
       " 'represnting': 96394,\n",
       " 'spews': 38877,\n",
       " 'compmus': 155894,\n",
       " 'noticethis': 96396,\n",
       " 'tavrian': 96397,\n",
       " '0\\xc2\\xb0': 96398,\n",
       " 'zeoh': 97942,\n",
       " 'recongnizes': 65776,\n",
       " 'editorsand': 153319,\n",
       " 'indricotherium': 96399,\n",
       " 'bra\\xc5\\x9fov': 96400,\n",
       " 'zoomie': 96401,\n",
       " 'azlan': 96402,\n",
       " 'pataliputra': 34378,\n",
       " 'softvision': 96404,\n",
       " 'zeor': 96405,\n",
       " \"fahy's\": 66220,\n",
       " 'obviousthe': 96407,\n",
       " 'kuifhoen': 96408,\n",
       " \"'arabian\": 96409,\n",
       " 'peremptory': 20231,\n",
       " 'dadoprso': 96410,\n",
       " '\\xe2\\x80\\x98summary\\xe2\\x80\\x99': 96411,\n",
       " 'mentors': 17538,\n",
       " 'theunbelieveabletruth': 96412,\n",
       " 'unnessery': 96413,\n",
       " 'wdpk': 65777,\n",
       " 'anupama': 65778,\n",
       " 'corporate': 3939,\n",
       " 'massaging': 65779,\n",
       " 'explanantion': 65780,\n",
       " 'bellow': 31555,\n",
       " 'willesden': 96415,\n",
       " 'absurdities': 44281,\n",
       " 'pa205': 65781,\n",
       " 'dhhcc': 96416,\n",
       " 'pa200': 65782,\n",
       " 'sdate': 153322,\n",
       " 'jonesboro': 45640,\n",
       " 'm\\xc3\\xbat': 96419,\n",
       " \"'reprimand'\": 157705,\n",
       " 'belloc': 65784,\n",
       " '\\xe2\\x80\\x9cradio': 96420,\n",
       " 'musicbrainz': 52317,\n",
       " 'immeadiately': 65785,\n",
       " 'kingdombaptist': 96421,\n",
       " 'shiit': 96422,\n",
       " 'duffer': 65786,\n",
       " 'advancement': 20232,\n",
       " 'shiia': 96423,\n",
       " 'mcnamara': 52318,\n",
       " 'gazimestan': 191234,\n",
       " \"mozart's\": 44282,\n",
       " '\\xe2\\x80\\x9csolvency\\xe2\\x80\\x9d': 65787,\n",
       " 'senecan': 96425,\n",
       " 'chemant': 96426,\n",
       " 'tcks': 191235,\n",
       " 'adviretisement': 96427,\n",
       " 'cascadingly': 203284,\n",
       " 'palahniuk': 169655,\n",
       " 'unequivocally': 14512,\n",
       " 'palazzo': 38878,\n",
       " 'indicative': 12472,\n",
       " 'xjklfdsjkl': 65789,\n",
       " 'ymb29': 34856,\n",
       " \"karmafist's\": 96430,\n",
       " 'henous': 96431,\n",
       " 'cosnsistently': 96432,\n",
       " 'sleuthing': 34857,\n",
       " 'acehall': 191237,\n",
       " '21217438': 96433,\n",
       " 'beauvais': 47966,\n",
       " 'victoria': 5382,\n",
       " 'hoaxtext': 153326,\n",
       " 'idusn0813601120101108': 96436,\n",
       " \"taxation'\": 96437,\n",
       " 'civilizuar': 96438,\n",
       " 'lacroix': 38879,\n",
       " 'gammal': 208350,\n",
       " 'aaaaaaaayui': 96439,\n",
       " 'crowd': 5190,\n",
       " 'crowe': 19468,\n",
       " 'yavana': 44283,\n",
       " 'crown': 4490,\n",
       " 'gabberfoxx': 96440,\n",
       " 'crows': 24385,\n",
       " 'tooken': 96441,\n",
       " \"soriano's\": 44284,\n",
       " 'gooble': 114771,\n",
       " 'emphases': 38880,\n",
       " \"'three\": 44285,\n",
       " 'crowz': 96444,\n",
       " 'assserts': 96445,\n",
       " 'phalangiotarbi': 96446,\n",
       " 'deragatory': 52319,\n",
       " 'perchance': 34858,\n",
       " 'tarangam': 96447,\n",
       " 'pileggi': 96448,\n",
       " 'lockdown': 44286,\n",
       " 'ichiu': 44287,\n",
       " 'spinelma': 65790,\n",
       " '3310473': 96449,\n",
       " 'supposebut': 96450,\n",
       " 'fruh': 96451,\n",
       " 'corfeball': 96452,\n",
       " 'frum': 96453,\n",
       " 'writecreole': 96454,\n",
       " 'indicare': 96455,\n",
       " 'completly': 12473,\n",
       " '\\xe2\\x80\\x9cfranchise\\xe2\\x80\\x9d': 96456,\n",
       " 'shwebo': 96457,\n",
       " 'fashid': 65791,\n",
       " \"'nowhereland'\": 96458,\n",
       " 'fashir': 52320,\n",
       " 'blatherskite': 96459,\n",
       " 'herria': 96460,\n",
       " 'visitng': 65792,\n",
       " 'saraab': 96461,\n",
       " 'societyno': 96462,\n",
       " '\\xe2\\x80\\x9cpersonality\\xe2\\x80\\x9d': 96464,\n",
       " 'spaceships': 65793,\n",
       " 'leominster': 96465,\n",
       " 'anomalous': 16997,\n",
       " 'exeception': 96466,\n",
       " 'phlanders': 96467,\n",
       " 'kellett': 96468,\n",
       " '20060119': 96469,\n",
       " 'gottahave': 96470,\n",
       " 'atheismapologist': 96471,\n",
       " 'vacarel\\xe2\\x80\\x99s': 96472,\n",
       " 'mangog': 96473,\n",
       " 'toyouji': 96474,\n",
       " 'marshall': 9124,\n",
       " 'honeymoon': 52321,\n",
       " \"university'\": 34859,\n",
       " 'anuslicker': 96475,\n",
       " 'hermitian': 52322,\n",
       " 'marshals': 38881,\n",
       " '\\xd0\\xafehevkor': 65794,\n",
       " 'mangos': 65795,\n",
       " 'aguments': 96476,\n",
       " 'fluegelman': 96477,\n",
       " 'despised': 18784,\n",
       " 'biddulph': 65796,\n",
       " 'fabric': 15599,\n",
       " \"sforza's\": 44263,\n",
       " \"khali's\": 96478,\n",
       " 'anoher': 96479,\n",
       " 'raped': 5383,\n",
       " 'pallikapu': 96480,\n",
       " 'grasping': 15189,\n",
       " 'despises': 65797,\n",
       " 'fabris': 96481,\n",
       " 'mentionability': 96482,\n",
       " '\\xc2\\xa01979\\xc2\\xa0\\xe2\\x86\\x92': 52323,\n",
       " 'rapes': 10213,\n",
       " 'raper': 38882,\n",
       " '5867': 96483,\n",
       " '5866': 96484,\n",
       " 'alexandrian': 34860,\n",
       " 'ubique': 44424,\n",
       " 'myhr': 96486,\n",
       " '\\xc4\\x80ry\\xc4\\x81varta': 66349,\n",
       " 'sortnumber': 52544,\n",
       " 'suryoyo': 32910,\n",
       " 'finucane': 131433,\n",
       " 'thesaurus': 38883,\n",
       " '2099996': 96490,\n",
       " 'abacha': 96491,\n",
       " 'bronies': 96492,\n",
       " \"hallow's\": 96493,\n",
       " 'congratulations': 2671,\n",
       " 'ii\\xe2\\x80\\x94usually': 187075,\n",
       " 'humbled': 23156,\n",
       " 'giv\\xc3\\xb3n': 65799,\n",
       " 'banyantree': 96494,\n",
       " \"rape'\": 44288,\n",
       " 'steping': 65800,\n",
       " 'humbles': 96495,\n",
       " 'nicest': 24386,\n",
       " 'mzilikazi': 17539,\n",
       " 'dvte': 190345,\n",
       " '2230': 82640,\n",
       " 'mirataeus': 65801,\n",
       " 'mercurynews': 96497,\n",
       " 'mgay': 96498,\n",
       " 'pricing': 15257,\n",
       " 'relabelling': 65802,\n",
       " 'passenger': 10790,\n",
       " 'ccinsider': 96500,\n",
       " 'moderns': 65803,\n",
       " 'rechecking': 52324,\n",
       " '\\xe2\\x80\\x94ricky': 96501,\n",
       " \"fs's\": 96502,\n",
       " '966197': 96503,\n",
       " 'slideshare': 96504,\n",
       " 'canadaeast': 96505,\n",
       " 'spacemusic': 39792,\n",
       " \"'characterising'\": 96507,\n",
       " 'cambodia': 13825,\n",
       " 'feculent': 96508,\n",
       " 'serous': 52325,\n",
       " 'mysidae': 178858,\n",
       " 'carbonaceous': 96509,\n",
       " 'daxter': 65804,\n",
       " 'oceanbeach': 96510,\n",
       " 'edon': 96511,\n",
       " 'extravascanza': 65805,\n",
       " 'bluffview': 65806,\n",
       " 'palms': 24387,\n",
       " \"modern'\": 96513,\n",
       " 'knowyourmeme': 96514,\n",
       " 'suwol': 65807,\n",
       " 'vmpu6wxkigd8qhmgigydw': 96515,\n",
       " 'palme': 96516,\n",
       " 'moresby': 65808,\n",
       " 'yellowstone': 19469,\n",
       " 'danski': 31902,\n",
       " 'reabsorption': 96517,\n",
       " 'neopentane': 96518,\n",
       " 'reftools': 96519,\n",
       " 'tameische': 114783,\n",
       " 'civiilzed': 96523,\n",
       " \"'''''someone\": 96524,\n",
       " 'poved': 27403,\n",
       " 'vmaxbike': 65809,\n",
       " 'davisjune': 44289,\n",
       " 'macrakides': 96525,\n",
       " 'hoineff': 96526,\n",
       " 'chaim': 31903,\n",
       " 'disassociate': 38884,\n",
       " 'whoever': 2370,\n",
       " 'shlomif': 96527,\n",
       " \"ultramarine's\": 96528,\n",
       " 'wooo': 146365,\n",
       " 'encyclotadd': 65810,\n",
       " 'rasterb': 52577,\n",
       " \"dositej's\": 96530,\n",
       " '20dies': 96531,\n",
       " 'chair': 5925,\n",
       " '5b43872721': 65811,\n",
       " 'entendre': 46957,\n",
       " 'macht': 44290,\n",
       " 'machu': 65812,\n",
       " 'ballet': 16998,\n",
       " 'amplification': 29441,\n",
       " 'baller': 96533,\n",
       " 'grappler': 96534,\n",
       " 'bubba73': 52326,\n",
       " \"apta's\": 96535,\n",
       " 'sangharaj': 65813,\n",
       " 'badgehuters': 96536,\n",
       " 'balled': 65814,\n",
       " 'grappled': 52327,\n",
       " 'ballen': 96537,\n",
       " 'macho': 27404,\n",
       " 'ramblesman': 65815,\n",
       " 'yourself82': 65816,\n",
       " 'flatbed': 96540,\n",
       " 'jerk': 3036,\n",
       " 'minuts': 96541,\n",
       " 'kososvo': 96542,\n",
       " 'olympus': 27405,\n",
       " 'gloomy': 34861,\n",
       " 'unpleasent': 65817,\n",
       " 'antiwikipedia': 79794,\n",
       " 'arbanites': 96544,\n",
       " 'dsylexic': 206221,\n",
       " 'exact': 1460,\n",
       " 'minute': 2692,\n",
       " 'ryan14': 96545,\n",
       " 'preuss': 65818,\n",
       " \"simab's\": 96546,\n",
       " 'holocauster': 96547,\n",
       " 'ppr17': 96548,\n",
       " '2029sbs6206k': 96549,\n",
       " \"saadat's\": 96550,\n",
       " 'illustrators': 65819,\n",
       " 'editwars': 52328,\n",
       " 'rcnaderlori': 96551,\n",
       " 'contribuitions': 96552,\n",
       " 'ientity': 171528,\n",
       " 'adorno': 52329,\n",
       " 'jewfaggotmongo': 44459,\n",
       " \"martha's\": 96555,\n",
       " 'wikified': 9865,\n",
       " 'joeyramoney': 96556,\n",
       " 'buur': 96557,\n",
       " \"con'e'wa'ga\": 96558,\n",
       " 'multicasting': 65820,\n",
       " 'loback': 96559,\n",
       " 'bagging': 96560,\n",
       " 'marchjuly': 96561,\n",
       " 'burck': 34862,\n",
       " 'antwerp': 31966,\n",
       " 'abdullah': 16013,\n",
       " 'louraine': 65821,\n",
       " \"'adaptive\": 96562,\n",
       " 'baggins': 52330,\n",
       " 'matras': 44292,\n",
       " \"'vlach'\": 96563,\n",
       " 'ground': 1976,\n",
       " 'i252': 96564,\n",
       " 'unintentionally': 13505,\n",
       " 'zrh': 65822,\n",
       " 'draftee': 96565,\n",
       " 'drafted': 9984,\n",
       " 'ufc': 5377,\n",
       " 'possiblility': 65823,\n",
       " 'oldies': 31904,\n",
       " 'operacion': 96567,\n",
       " 'gendercide': 65824,\n",
       " 'tokohbaka': 96568,\n",
       " 'contributuons': 96570,\n",
       " 'id353879860june': 65825,\n",
       " 'owuld': 96571,\n",
       " 'dangeour': 96572,\n",
       " 'benson': 19470,\n",
       " 'fledge': 65826,\n",
       " 'fasri': 44293,\n",
       " 'renata3': 44294,\n",
       " 'cusack': 66485,\n",
       " 'groupstyle': 96574,\n",
       " 'expressively': 96575,\n",
       " 'booooooom': 96576,\n",
       " 'milborneone': 29442,\n",
       " 'teamworking': 96577,\n",
       " 'gidday': 65827,\n",
       " 'norcrossmedia': 191254,\n",
       " 'pinktentacle': 96578,\n",
       " 'europ\\xc3\\xa9enne': 65828,\n",
       " 'fakers': 52331,\n",
       " 'mysa050107': 96582,\n",
       " 'wrightie99': 96583,\n",
       " 'fakery': 96584,\n",
       " 'l\\xc3\\xa9ger': 96585,\n",
       " 'kimantinya': 96586,\n",
       " 'iberians': 65829,\n",
       " 'handbags': 52332,\n",
       " 'perished': 20233,\n",
       " 'pints': 65830,\n",
       " 'gulags': 65831,\n",
       " 'kallamata': 96587,\n",
       " \"j'inciterai\": 119091,\n",
       " 'vigour': 38885,\n",
       " 'ticaret': 96588,\n",
       " 'annunciate': 96589,\n",
       " 'itselfthe': 96590,\n",
       " \"poincare'\": 96591,\n",
       " 'wernotwiki': 96592,\n",
       " \"cats'\": 96594,\n",
       " 'varient': 52333,\n",
       " 'ooooooo': 96595,\n",
       " 'oooooon': 96596,\n",
       " 'ooooooh': 52334,\n",
       " 'approving': 14838,\n",
       " 'cats1': 96597,\n",
       " 'bharata': 31905,\n",
       " 'jess2234': 96598,\n",
       " \"'proof'\": 52335,\n",
       " 'bharati': 96599,\n",
       " 'cardinality': 27406,\n",
       " 'zehtabi': 96600,\n",
       " 'ayway': 96601,\n",
       " 'egern': 96602,\n",
       " 'ctka': 96603,\n",
       " 'groverthegnome': 96604,\n",
       " 'duralumium': 96605,\n",
       " 'rutherford': 16482,\n",
       " 'barning': 96606,\n",
       " 'tashvikaya': 96607,\n",
       " 'feleings': 199053,\n",
       " 'arbitor': 96608,\n",
       " 'following': 423,\n",
       " 'misunerstand': 96609,\n",
       " '070208': 96610,\n",
       " 'erachima': 96611,\n",
       " 'coraz\\xc3\\xb3n': 65832,\n",
       " 'stetson': 34863,\n",
       " \"proponent's\": 65833,\n",
       " \"'adm'\": 96612,\n",
       " \"ginny's\": 96613,\n",
       " 'burgerist': 96614,\n",
       " 'especcially': 65834,\n",
       " \"heard'\": 96615,\n",
       " 'finkelman': 34864,\n",
       " 'reqire': 96616,\n",
       " 'litre': 29443,\n",
       " 'repied': 96617,\n",
       " 'greenberg\\xe2\\x80\\x99s': 96618,\n",
       " 'confermed': 96619,\n",
       " 'thanking': 11652,\n",
       " 'litru': 96620,\n",
       " 'popbitch': 82199,\n",
       " 'commentslittle': 96622,\n",
       " 'proselytising': 65835,\n",
       " 'mao\\xe2\\x80\\x99s': 203418,\n",
       " \"silva's\": 96623,\n",
       " 'epidemiologic': 183779,\n",
       " 'epidemiologia': 96624,\n",
       " 'whitewriter': 96625,\n",
       " 'fueled': 21116,\n",
       " 'echidna': 44295,\n",
       " \"imagesit's\": 96626,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t = pad_sequences(list_tokenized_train, maxlen=max_len)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0090650525, 0.44281197)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_unique_words, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_unique_words: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 200)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0, batch_size=512)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/sklearn/model_selection/_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "[X_tra, X_val, y_tra, y_val] = train_test_split(X_t, y_train.values, train_size=0.90, random_state=233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143613, 250)\n",
      "(15958, 250)\n"
     ]
    }
   ],
   "source": [
    "print(X_tra.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(len(X_tra)/batch_size) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gru = Sequential()\n",
    "gru.add(Embedding(input_dim=max_unique_words, output_dim=embed_size, weights=[embedding_matrix], trainable=False,\n",
    "                 input_length=max_len)),\n",
    "gru.add(Bidirectional(GRU(256, return_sequences=True,name='gru_1', dropout_U=0.1, dropout_W=0.1)))\n",
    "gru.add(GlobalMaxPooling1D())\n",
    "gru.add(Dropout(0.3))\n",
    "gru.add(Dense(128, activation=\"relu\"))\n",
    "gru.add(Dropout(0.1))\n",
    "#gru.add(BatchNormalization())\n",
    "gru.add(Dense(6, activation=\"sigmoid\"))\n",
    "#gru.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu', \n",
    "                      #input_length=(None, None, 128)))\n",
    "#gru.add(MaxPooling1D(pool_length=2, stride=2))\n",
    "#gru.add(Flatten())\n",
    "#gru.add(Dropout(0.5))\n",
    "#gru.add(Dense(128, activation=\"relu\"))\n",
    "#gru.add(Dropout(0.1))\n",
    "#gru.add(Dense(6, activation=\"sigmoid\"))\n",
    "gru.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "tensorboard = TensorBoard(log_dir='./logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_init, lr_fin = 0.001, 0.0005\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "K.set_value(gru.optimizer.lr, lr_init)\n",
    "K.set_value(gru.optimizer.decay, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 250, 200)      20000000    embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 250, 256)      252672      embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "globalmaxpooling1d_2 (GlobalMaxP (None, 256)           0           bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 256)           0           globalmaxpooling1d_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           32896       dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 6)             774         dropout_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 20,286,342\n",
      "Trainable params: 286,342\n",
      "Non-trainable params: 20,000,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# current model architecture\n",
    "gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path=\"weights_gru_256BiGru_dropouts0pt1_dense128_BS512_expdecay_adam.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks_list = [checkpoint, early, RocAuc] #early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/100\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9775Epoch 00000: val_loss improved from inf to 0.04718, saving model to weights_gru_256BiGru_dropouts0pt1_dense128_BS512_expdecay_adam.best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.979955 \n",
      "\n",
      "143613/143613 [==============================] - 427s - loss: 0.0657 - acc: 0.9775 - val_loss: 0.0472 - val_acc: 0.9827\n",
      "Epoch 2/100\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9825Epoch 00001: val_loss improved from 0.04718 to 0.04429, saving model to weights_gru_256BiGru_dropouts0pt1_dense128_BS512_expdecay_adam.best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.985594 \n",
      "\n",
      "143613/143613 [==============================] - 427s - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 3/100\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9835Epoch 00002: val_loss improved from 0.04429 to 0.04223, saving model to weights_gru_256BiGru_dropouts0pt1_dense128_BS512_expdecay_adam.best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.987980 \n",
      "\n",
      "143613/143613 [==============================] - 427s - loss: 0.0426 - acc: 0.9835 - val_loss: 0.0422 - val_acc: 0.9839\n",
      "Epoch 4/100\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9844Epoch 00003: val_loss improved from 0.04223 to 0.04154, saving model to weights_gru_256BiGru_dropouts0pt1_dense128_BS512_expdecay_adam.best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.988523 \n",
      "\n",
      "143613/143613 [==============================] - 429s - loss: 0.0401 - acc: 0.9844 - val_loss: 0.0415 - val_acc: 0.9842\n",
      "Epoch 5/100\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9849Epoch 00004: val_loss improved from 0.04154 to 0.04132, saving model to weights_gru_256BiGru_dropouts0pt1_dense128_BS512_expdecay_adam.best.hdf5\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.988642 \n",
      "\n",
      "143613/143613 [==============================] - 436s - loss: 0.0384 - acc: 0.9849 - val_loss: 0.0413 - val_acc: 0.9844\n",
      "Epoch 6/100\n",
      "143360/143613 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00005: val_loss did not improve\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-61359f0434b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m gru.fit(X_tra,y_tra, batch_size=batch_size, nb_epoch=epochs, verbose=1, validation_data=(X_val, y_val), \n\u001b[0;32m----> 2\u001b[0;31m         callbacks = callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    909\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-d6d4e4647bae>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1272\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1943\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/sarthak/HDD/Anaconda/envs/riskmethods_27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gru.fit(X_tra,y_tra, batch_size=batch_size, nb_epoch=epochs, verbose=1, validation_data=(X_val, y_val), \n",
    "        callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 153s   \n"
     ]
    }
   ],
   "source": [
    "gru.load_weights(file_path)\n",
    "y_test = gru.predict(X_te, verbose=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.98168468e-01,   3.63719732e-01,   9.73769248e-01,\n",
       "          5.31068370e-02,   8.98709536e-01,   5.89170694e-01],\n",
       "       [  3.09806666e-04,   9.59095061e-08,   1.26369432e-05,\n",
       "          6.25580014e-07,   2.91981560e-05,   3.43879009e-07],\n",
       "       [  1.11715388e-04,   2.69454148e-07,   9.54684856e-06,\n",
       "          1.35619655e-06,   9.94454422e-06,   1.07646292e-06],\n",
       "       ..., \n",
       "       [  6.85018022e-05,   1.63396638e-08,   7.76599609e-06,\n",
       "          9.47088168e-08,   4.81047482e-06,   2.03495404e-07],\n",
       "       [  3.96419870e-04,   2.26479429e-07,   1.74015677e-05,\n",
       "          1.64334813e-06,   1.89090206e-05,   4.82305222e-05],\n",
       "       [  9.80085313e-01,   1.19893076e-02,   7.55342305e-01,\n",
       "          9.66226216e-04,   5.15134931e-01,   2.17875186e-03]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission[list_classes] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.998168</td>\n",
       "      <td>3.637197e-01</td>\n",
       "      <td>0.973769</td>\n",
       "      <td>5.310684e-02</td>\n",
       "      <td>0.898710</td>\n",
       "      <td>5.891707e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>9.590951e-08</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>6.255800e-07</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>3.438790e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>2.694541e-07</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.356197e-06</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.076463e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>1.863934e-07</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>2.157662e-06</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>6.031547e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>1.493587e-06</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>2.831962e-05</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>3.234728e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>1.122055e-07</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>7.131162e-07</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>4.506473e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>1.357754e-07</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1.255489e-06</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>1.661784e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>0.237570</td>\n",
       "      <td>8.948431e-05</td>\n",
       "      <td>0.011007</td>\n",
       "      <td>1.152769e-04</td>\n",
       "      <td>0.018218</td>\n",
       "      <td>1.560999e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>0.029724</td>\n",
       "      <td>4.850974e-06</td>\n",
       "      <td>0.002690</td>\n",
       "      <td>4.336417e-05</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>6.530054e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>2.171362e-07</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>2.698659e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>1.051280e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002eadc3b301559</td>\n",
       "      <td>0.323649</td>\n",
       "      <td>6.741754e-05</td>\n",
       "      <td>0.087605</td>\n",
       "      <td>2.790885e-05</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>1.029223e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>0.163931</td>\n",
       "      <td>2.541314e-04</td>\n",
       "      <td>0.031046</td>\n",
       "      <td>8.416433e-04</td>\n",
       "      <td>0.016182</td>\n",
       "      <td>1.714725e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003806b11932181</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>7.990052e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>8.281796e-07</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>2.274603e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>1.322626e-08</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.298808e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.053247e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>1.164254e-08</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>8.578174e-08</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>8.837300e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000634272d0d44eb</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>7.521479e-08</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>4.466775e-07</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>3.645353e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>000663aff0fffc80</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>3.889504e-05</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>1.045754e-04</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>6.538627e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>000689dd34e20979</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>1.329063e-07</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.231394e-06</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>4.859549e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>000834769115370c</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>3.153081e-08</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>6.752531e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3.413022e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>000844b52dee5f3f</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>1.179698e-05</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>4.431115e-05</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>9.295216e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>00084da5d4ead7aa</td>\n",
       "      <td>0.010581</td>\n",
       "      <td>2.720545e-05</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>1.452383e-04</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>6.959421e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>00091c35fa9d0465</td>\n",
       "      <td>0.832750</td>\n",
       "      <td>1.152799e-02</td>\n",
       "      <td>0.084431</td>\n",
       "      <td>8.533468e-02</td>\n",
       "      <td>0.123415</td>\n",
       "      <td>2.889743e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>000968ce11f5ee34</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>1.909462e-05</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>2.888440e-04</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>4.249008e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0009734200a85047</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>2.399319e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.263064e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>5.823612e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>00097b6214686db5</td>\n",
       "      <td>0.126769</td>\n",
       "      <td>2.707519e-05</td>\n",
       "      <td>0.009643</td>\n",
       "      <td>1.058236e-04</td>\n",
       "      <td>0.004343</td>\n",
       "      <td>2.378647e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0009aef4bd9e1697</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>1.441118e-08</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>1.010816e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.725655e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>000a02d807ae0254</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>4.348012e-08</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>4.139918e-07</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>5.002851e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>000a6c6d4e89b9bc</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>2.786632e-06</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>4.703618e-06</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>2.100400e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>000bafe2080bba82</td>\n",
       "      <td>0.230094</td>\n",
       "      <td>2.254437e-04</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>9.323322e-04</td>\n",
       "      <td>0.015562</td>\n",
       "      <td>8.975495e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>000bf0a9894b2807</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>2.216832e-08</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>3.077533e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>4.142402e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153134</th>\n",
       "      <td>fff3ae2e177b6bb3</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>1.188481e-08</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.624802e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.442050e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153135</th>\n",
       "      <td>fff4109e837f7acc</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>1.940158e-07</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>9.984036e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>7.867503e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153136</th>\n",
       "      <td>fff4373a81ef9f2a</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>1.596916e-08</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.245974e-07</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>1.785725e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153137</th>\n",
       "      <td>fff460574ddbcd80</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>1.753254e-06</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>4.218385e-05</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>8.623402e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153138</th>\n",
       "      <td>fff4fc0a1555be5c</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>2.535852e-07</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>1.690768e-06</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>1.586767e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153139</th>\n",
       "      <td>fff5b9bb944d634c</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>4.684833e-08</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.882260e-07</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>7.707139e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153140</th>\n",
       "      <td>fff5c4a77fe0c05f</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>6.660548e-08</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.026143e-07</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2.425996e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153141</th>\n",
       "      <td>fff5fb61bd637c82</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>3.724723e-08</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>7.128660e-07</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>3.854610e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153142</th>\n",
       "      <td>fff69311f306df44</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>5.064290e-07</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>2.085628e-06</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>2.545025e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153143</th>\n",
       "      <td>fff6ad63666fb304</td>\n",
       "      <td>0.997533</td>\n",
       "      <td>1.765803e-01</td>\n",
       "      <td>0.984488</td>\n",
       "      <td>6.763177e-04</td>\n",
       "      <td>0.641691</td>\n",
       "      <td>7.590056e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153144</th>\n",
       "      <td>fff7159b3ee95618</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>3.110660e-06</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>1.970188e-05</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>1.592439e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153145</th>\n",
       "      <td>fff718ffe5f05559</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>2.845237e-08</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>2.472405e-07</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.296219e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153146</th>\n",
       "      <td>fff7fc22a0cdccd3</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>3.726727e-08</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4.608412e-07</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3.229570e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153147</th>\n",
       "      <td>fff83b80284d8440</td>\n",
       "      <td>0.055467</td>\n",
       "      <td>3.438755e-05</td>\n",
       "      <td>0.006854</td>\n",
       "      <td>1.062223e-04</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>6.259501e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153148</th>\n",
       "      <td>fff8ef316d0c6990</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>1.983340e-07</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>7.542747e-07</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>4.365014e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153149</th>\n",
       "      <td>fff8f521a7dbcd47</td>\n",
       "      <td>0.805586</td>\n",
       "      <td>9.428813e-03</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>1.793462e-02</td>\n",
       "      <td>0.232800</td>\n",
       "      <td>4.246650e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153150</th>\n",
       "      <td>fff8f64043129fa2</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>8.949867e-08</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>2.507738e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>2.219789e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153151</th>\n",
       "      <td>fff9d70fe0722906</td>\n",
       "      <td>0.826167</td>\n",
       "      <td>3.300277e-02</td>\n",
       "      <td>0.593326</td>\n",
       "      <td>4.668633e-03</td>\n",
       "      <td>0.627457</td>\n",
       "      <td>5.639490e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153152</th>\n",
       "      <td>fff9fa508f400ee6</td>\n",
       "      <td>0.581366</td>\n",
       "      <td>1.528773e-03</td>\n",
       "      <td>0.376719</td>\n",
       "      <td>2.706817e-04</td>\n",
       "      <td>0.034296</td>\n",
       "      <td>5.230593e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153153</th>\n",
       "      <td>fffa3fae1890b40a</td>\n",
       "      <td>0.756865</td>\n",
       "      <td>4.265930e-02</td>\n",
       "      <td>0.664312</td>\n",
       "      <td>6.614581e-02</td>\n",
       "      <td>0.246610</td>\n",
       "      <td>7.328771e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153154</th>\n",
       "      <td>fffa8a11c4378854</td>\n",
       "      <td>0.675586</td>\n",
       "      <td>1.923706e-03</td>\n",
       "      <td>0.022957</td>\n",
       "      <td>5.510141e-03</td>\n",
       "      <td>0.086710</td>\n",
       "      <td>3.024308e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153155</th>\n",
       "      <td>fffac2a094c8e0e2</td>\n",
       "      <td>0.989422</td>\n",
       "      <td>1.148816e-01</td>\n",
       "      <td>0.908717</td>\n",
       "      <td>4.829424e-03</td>\n",
       "      <td>0.860532</td>\n",
       "      <td>7.198045e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153156</th>\n",
       "      <td>fffb5451268fb5ba</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>3.512231e-06</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>3.380531e-05</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>1.449758e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153157</th>\n",
       "      <td>fffc2b34bbe61c8d</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>3.136491e-07</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>2.156683e-06</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>7.726217e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153158</th>\n",
       "      <td>fffc489742ffe69b</td>\n",
       "      <td>0.872748</td>\n",
       "      <td>4.244219e-03</td>\n",
       "      <td>0.192014</td>\n",
       "      <td>3.827381e-04</td>\n",
       "      <td>0.735297</td>\n",
       "      <td>3.530183e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>0.620499</td>\n",
       "      <td>1.695158e-04</td>\n",
       "      <td>0.117336</td>\n",
       "      <td>5.718814e-05</td>\n",
       "      <td>0.025973</td>\n",
       "      <td>2.727534e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>0.027480</td>\n",
       "      <td>2.747683e-05</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>1.379104e-03</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>4.178378e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>1.633966e-08</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>9.470882e-08</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2.034954e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>2.264794e-07</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>1.643348e-06</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>4.823052e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>0.980085</td>\n",
       "      <td>1.198931e-02</td>\n",
       "      <td>0.755342</td>\n",
       "      <td>9.662262e-04</td>\n",
       "      <td>0.515135</td>\n",
       "      <td>2.178752e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id     toxic  severe_toxic   obscene        threat  \\\n",
       "0       00001cee341fdb12  0.998168  3.637197e-01  0.973769  5.310684e-02   \n",
       "1       0000247867823ef7  0.000310  9.590951e-08  0.000013  6.255800e-07   \n",
       "2       00013b17ad220c46  0.000112  2.694541e-07  0.000010  1.356197e-06   \n",
       "3       00017563c3f7919a  0.000346  1.863934e-07  0.000056  2.157662e-06   \n",
       "4       00017695ad8997eb  0.002477  1.493587e-06  0.000089  2.831962e-05   \n",
       "5       0001ea8717f6de06  0.000174  1.122055e-07  0.000021  7.131162e-07   \n",
       "6       00024115d4cbde0f  0.001091  1.357754e-07  0.000049  1.255489e-06   \n",
       "7       000247e83dcc1211  0.237570  8.948431e-05  0.011007  1.152769e-04   \n",
       "8       00025358d4737918  0.029724  4.850974e-06  0.002690  4.336417e-05   \n",
       "9       00026d1092fe71cc  0.000398  2.171362e-07  0.000037  2.698659e-06   \n",
       "10      0002eadc3b301559  0.323649  6.741754e-05  0.087605  2.790885e-05   \n",
       "11      0002f87b16116a7f  0.163931  2.541314e-04  0.031046  8.416433e-04   \n",
       "12      0003806b11932181  0.000382  7.990052e-08  0.000020  8.281796e-07   \n",
       "13      0003e1cccfd5a40a  0.000062  1.322626e-08  0.000004  1.298808e-07   \n",
       "14      00059ace3e3e9a53  0.000068  1.164254e-08  0.000006  8.578174e-08   \n",
       "15      000634272d0d44eb  0.000163  7.521479e-08  0.000013  4.466775e-07   \n",
       "16      000663aff0fffc80  0.007684  3.889504e-05  0.001128  1.045754e-04   \n",
       "17      000689dd34e20979  0.001932  1.329063e-07  0.000076  1.231394e-06   \n",
       "18      000834769115370c  0.000217  3.153081e-08  0.000010  6.752531e-07   \n",
       "19      000844b52dee5f3f  0.006136  1.179698e-05  0.000587  4.431115e-05   \n",
       "20      00084da5d4ead7aa  0.010581  2.720545e-05  0.001517  1.452383e-04   \n",
       "21      00091c35fa9d0465  0.832750  1.152799e-02  0.084431  8.533468e-02   \n",
       "22      000968ce11f5ee34  0.021025  1.909462e-05  0.000833  2.888440e-04   \n",
       "23      0009734200a85047  0.000153  2.399319e-08  0.000007  3.263064e-07   \n",
       "24      00097b6214686db5  0.126769  2.707519e-05  0.009643  1.058236e-04   \n",
       "25      0009aef4bd9e1697  0.000086  1.441118e-08  0.000009  1.010816e-07   \n",
       "26      000a02d807ae0254  0.000163  4.348012e-08  0.000008  4.139918e-07   \n",
       "27      000a6c6d4e89b9bc  0.009657  2.786632e-06  0.000637  4.703618e-06   \n",
       "28      000bafe2080bba82  0.230094  2.254437e-04  0.002140  9.323322e-04   \n",
       "29      000bf0a9894b2807  0.000168  2.216832e-08  0.000010  3.077533e-07   \n",
       "...                  ...       ...           ...       ...           ...   \n",
       "153134  fff3ae2e177b6bb3  0.000078  1.188481e-08  0.000005  1.624802e-07   \n",
       "153135  fff4109e837f7acc  0.000471  1.940158e-07  0.000027  9.984036e-07   \n",
       "153136  fff4373a81ef9f2a  0.000068  1.596916e-08  0.000006  1.245974e-07   \n",
       "153137  fff460574ddbcd80  0.004544  1.753254e-06  0.000167  4.218385e-05   \n",
       "153138  fff4fc0a1555be5c  0.000896  2.535852e-07  0.000073  1.690768e-06   \n",
       "153139  fff5b9bb944d634c  0.000200  4.684833e-08  0.000015  1.882260e-07   \n",
       "153140  fff5c4a77fe0c05f  0.000121  6.660548e-08  0.000007  9.026143e-07   \n",
       "153141  fff5fb61bd637c82  0.000232  3.724723e-08  0.000009  7.128660e-07   \n",
       "153142  fff69311f306df44  0.000482  5.064290e-07  0.000070  2.085628e-06   \n",
       "153143  fff6ad63666fb304  0.997533  1.765803e-01  0.984488  6.763177e-04   \n",
       "153144  fff7159b3ee95618  0.005590  3.110660e-06  0.000388  1.970188e-05   \n",
       "153145  fff718ffe5f05559  0.000162  2.845237e-08  0.000008  2.472405e-07   \n",
       "153146  fff7fc22a0cdccd3  0.000108  3.726727e-08  0.000012  4.608412e-07   \n",
       "153147  fff83b80284d8440  0.055467  3.438755e-05  0.006854  1.062223e-04   \n",
       "153148  fff8ef316d0c6990  0.000814  1.983340e-07  0.000030  7.542747e-07   \n",
       "153149  fff8f521a7dbcd47  0.805586  9.428813e-03  0.090388  1.793462e-02   \n",
       "153150  fff8f64043129fa2  0.000459  8.949867e-08  0.000020  2.507738e-06   \n",
       "153151  fff9d70fe0722906  0.826167  3.300277e-02  0.593326  4.668633e-03   \n",
       "153152  fff9fa508f400ee6  0.581366  1.528773e-03  0.376719  2.706817e-04   \n",
       "153153  fffa3fae1890b40a  0.756865  4.265930e-02  0.664312  6.614581e-02   \n",
       "153154  fffa8a11c4378854  0.675586  1.923706e-03  0.022957  5.510141e-03   \n",
       "153155  fffac2a094c8e0e2  0.989422  1.148816e-01  0.908717  4.829424e-03   \n",
       "153156  fffb5451268fb5ba  0.009706  3.512231e-06  0.000618  3.380531e-05   \n",
       "153157  fffc2b34bbe61c8d  0.003652  3.136491e-07  0.000155  2.156683e-06   \n",
       "153158  fffc489742ffe69b  0.872748  4.244219e-03  0.192014  3.827381e-04   \n",
       "153159  fffcd0960ee309b5  0.620499  1.695158e-04  0.117336  5.718814e-05   \n",
       "153160  fffd7a9a6eb32c16  0.027480  2.747683e-05  0.000927  1.379104e-03   \n",
       "153161  fffda9e8d6fafa9e  0.000069  1.633966e-08  0.000008  9.470882e-08   \n",
       "153162  fffe8f1340a79fc2  0.000396  2.264794e-07  0.000017  1.643348e-06   \n",
       "153163  ffffce3fb183ee80  0.980085  1.198931e-02  0.755342  9.662262e-04   \n",
       "\n",
       "          insult  identity_hate  \n",
       "0       0.898710   5.891707e-01  \n",
       "1       0.000029   3.438790e-07  \n",
       "2       0.000010   1.076463e-06  \n",
       "3       0.000057   6.031547e-07  \n",
       "4       0.000093   3.234728e-06  \n",
       "5       0.000021   4.506473e-07  \n",
       "6       0.000102   1.661784e-06  \n",
       "7       0.018218   1.560999e-04  \n",
       "8       0.003569   6.530054e-05  \n",
       "9       0.000029   1.051280e-06  \n",
       "10      0.009836   1.029223e-04  \n",
       "11      0.016182   1.714725e-02  \n",
       "12      0.000027   2.274603e-06  \n",
       "13      0.000004   2.053247e-07  \n",
       "14      0.000006   8.837300e-08  \n",
       "15      0.000016   3.645353e-07  \n",
       "16      0.001370   6.538627e-05  \n",
       "17      0.000083   4.859549e-06  \n",
       "18      0.000009   3.413022e-07  \n",
       "19      0.000620   9.295216e-06  \n",
       "20      0.001140   6.959421e-05  \n",
       "21      0.123415   2.889743e-01  \n",
       "22      0.001101   4.249008e-04  \n",
       "23      0.000008   5.823612e-07  \n",
       "24      0.004343   2.378647e-03  \n",
       "25      0.000006   1.725655e-07  \n",
       "26      0.000012   5.002851e-07  \n",
       "27      0.001101   2.100400e-05  \n",
       "28      0.015562   8.975495e-02  \n",
       "29      0.000013   4.142402e-07  \n",
       "...          ...            ...  \n",
       "153134  0.000005   1.442050e-07  \n",
       "153135  0.000028   7.867503e-06  \n",
       "153136  0.000005   1.785725e-07  \n",
       "153137  0.000312   8.623402e-06  \n",
       "153138  0.000064   1.586767e-06  \n",
       "153139  0.000016   7.707139e-07  \n",
       "153140  0.000009   2.425996e-07  \n",
       "153141  0.000011   3.854610e-07  \n",
       "153142  0.000049   2.545025e-06  \n",
       "153143  0.641691   7.590056e-03  \n",
       "153144  0.000618   1.592439e-05  \n",
       "153145  0.000011   4.296219e-06  \n",
       "153146  0.000007   3.229570e-07  \n",
       "153147  0.003853   6.259501e-05  \n",
       "153148  0.000050   4.365014e-06  \n",
       "153149  0.232800   4.246650e-01  \n",
       "153150  0.000018   2.219789e-06  \n",
       "153151  0.627457   5.639490e-02  \n",
       "153152  0.034296   5.230593e-04  \n",
       "153153  0.246610   7.328771e-03  \n",
       "153154  0.086710   3.024308e-01  \n",
       "153155  0.860532   7.198045e-01  \n",
       "153156  0.000518   1.449758e-04  \n",
       "153157  0.000354   7.726217e-06  \n",
       "153158  0.735297   3.530183e-03  \n",
       "153159  0.025973   2.727534e-04  \n",
       "153160  0.001425   4.178378e-04  \n",
       "153161  0.000005   2.034954e-07  \n",
       "153162  0.000019   4.823052e-05  \n",
       "153163  0.515135   2.178752e-03  \n",
       "\n",
       "[153164 rows x 7 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"gru_13.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
